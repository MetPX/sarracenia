<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Sarracenia Status January 2018 &mdash; Sarracenia 3.00.54rc3 documentation</title>
      <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=80d5e7a1" />
      <link rel="stylesheet" type="text/css" href="../../_static/css/theme.css?v=19f00094" />

  
    <link rel="shortcut icon" href="../../_static/sarra_horror_culture_favicon.png"/>
  <!--[if lt IE 9]>
    <script src="../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="../../_static/jquery.js?v=5d32c60e"></script>
        <script src="../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
        <script src="../../_static/documentation_options.js?v=0a7b563c"></script>
        <script src="../../_static/doctools.js?v=9a2dae69"></script>
        <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script src="https://code.jquery.com/jquery-3.6.0.min.js"></script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="History/Context of Sarracenia" href="Evolution.html" />
    <link rel="prev" title="History" href="index.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../index.html" class="icon icon-home">
            Sarracenia
              <img src="../../_static/sarra_horror_culture_w200.png" class="logo" alt="Logo"/>
          </a>
              <div class="version">
                3.00.54rc3
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../Overview.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Tutorials/index.html">Tutorials</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../How2Guides/index.html">HOWTOS</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../index.html">Explanation</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../Overview.html">Overview</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Concepts.html">General Sarracenia Concepts</a></li>
<li class="toctree-l2"><a class="reference internal" href="../CommandLineGuide.html">Command Line Guide</a></li>
<li class="toctree-l2"><a class="reference internal" href="../SarraPluginDev.html">Sarracenia Programming Guide</a></li>
<li class="toctree-l2"><a class="reference internal" href="../DeploymentConsiderations.html">Deployment Considerations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../DetectFileReady.html">File Detection Strategies</a></li>
<li class="toctree-l2"><a class="reference internal" href="../DuplicateSuppression.html">Duplicate Suppression</a></li>
<li class="toctree-l2"><a class="reference internal" href="../FileCompletion.html">Delivery Completion (inflight)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Glossary.html">Glossary</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="index.html">History</a><ul class="current">
<li class="toctree-l3 current"><a class="current reference internal" href="#">Sarracenia Status January 2018</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#comparison-to-2015-video">Comparison to 2015 Video</a></li>
<li class="toctree-l4"><a class="reference internal" href="#central-data-flows">Central Data Flows</a></li>
<li class="toctree-l4"><a class="reference internal" href="#weather-application-flows">Weather Application Flows</a></li>
<li class="toctree-l4"><a class="reference internal" href="#goes-r-acquisition">GOES-R Acquisition</a></li>
<li class="toctree-l4"><a class="reference internal" href="#hpc-acquisition-feeds">HPC Acquisition Feeds</a></li>
<li class="toctree-l4"><a class="reference internal" href="#radar-data-flows">RADAR Data Flows</a></li>
<li class="toctree-l4"><a class="reference internal" href="#hpc-mirroring">HPC Mirroring</a></li>
<li class="toctree-l4"><a class="reference internal" href="#application-changes-in-2017">Application Changes in 2017</a></li>
<li class="toctree-l4"><a class="reference internal" href="#coming-in-2018">Coming in 2018</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="Evolution.html">History/Context of Sarracenia</a></li>
<li class="toctree-l3"><a class="reference internal" href="HPC_Mirroring_Use_Case.html">Case Study: HPC Mirroring</a></li>
<li class="toctree-l3"><a class="reference internal" href="mesh_gts.html">Mesh-Style Data Exchange for the WIS-GTS in 2019</a></li>
<li class="toctree-l3"><a class="reference internal" href="messages_v01.html">Message v01 Format</a></li>
<li class="toctree-l3"><a class="reference internal" href="messages_v02.html">Description of the message v02 protocol / format</a></li>
<li class="toctree-l3"><a class="reference internal" href="messages_v03.html">Changes Made to create v03</a></li>
<li class="toctree-l3"><a class="reference internal" href="sr3_Announcement.html">Announcing Sr3</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../sftps.html">Why SFTP is More Often Chosen than FTPS</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Sundew_Migration/index.html">Sundew Migration Guide</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../Reference/index.html">Reference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Contribution/index.html">Contributing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api-documentation.html">API Documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../fr/index.html">En français</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">Sarracenia</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../index.html">Explanation</a></li>
          <li class="breadcrumb-item"><a href="index.html">History</a></li>
      <li class="breadcrumb-item active">Sarracenia Status January 2018</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/Explanation/History/deployment_2018.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="sarracenia-status-january-2018">
<h1><a class="toc-backref" href="#id1" role="doc-backlink">Sarracenia Status January 2018</a><a class="headerlink" href="#sarracenia-status-january-2018" title="Link to this heading"></a></h1>
<p>[ <a class="reference external" href="fr/deployment_2018.rst">version française</a> ]</p>
<p>Sarracenia is a small application iteratively developed by addressing one use
case at a time, so development and deployment have been inextricably linked up
to this point. That iterative process precipitated changes in the core of the
application which have made it something of a moving target until now. In
January 2018, the application has reached the point where all intended use cases
are addressed by the application core. In the coming year, the emphasis will be
on facilitating on-boarding, development of some derived services, and
deploying the newly complete application more generally.</p>
<nav class="contents" id="contents">
<p class="topic-title">Contents</p>
<ul class="simple">
<li><p><a class="reference internal" href="#sarracenia-status-january-2018" id="id1">Sarracenia Status January 2018</a></p>
<ul>
<li><p><a class="reference internal" href="#comparison-to-2015-video" id="id2">Comparison to 2015 Video</a></p></li>
<li><p><a class="reference internal" href="#central-data-flows" id="id3">Central Data Flows</a></p></li>
<li><p><a class="reference internal" href="#weather-application-flows" id="id4">Weather Application Flows</a></p></li>
<li><p><a class="reference internal" href="#goes-r-acquisition" id="id5">GOES-R Acquisition</a></p></li>
<li><p><a class="reference internal" href="#hpc-acquisition-feeds" id="id6">HPC Acquisition Feeds</a></p></li>
<li><p><a class="reference internal" href="#radar-data-flows" id="id7">RADAR Data Flows</a></p></li>
<li><p><a class="reference internal" href="#hpc-mirroring" id="id8">HPC Mirroring</a></p></li>
<li><p><a class="reference internal" href="#application-changes-in-2017" id="id9">Application Changes in 2017</a></p></li>
<li><p><a class="reference internal" href="#coming-in-2018" id="id10">Coming in 2018</a></p></li>
</ul>
</li>
</ul>
</nav>
<section id="comparison-to-2015-video">
<h2><a class="toc-backref" href="#id2" role="doc-backlink">Comparison to 2015 Video</a><a class="headerlink" href="#comparison-to-2015-video" title="Link to this heading"></a></h2>
<p>The November 2015 video ( <a class="reference external" href="https://www.youtube.com/watch?v=G47DRwzwckk">Sarracenia in 10 Minutes</a> )
outlined a vision. First phase of development work occurred in 2015 and early
2016, followed by important deployments later in 2016. This update,
written in early 2018, explores progress made mostly in 2017.</p>
<p>Use cases mentioned in the video which were implemented:</p>
<ul class="simple">
<li><p>Central meteorological pumping has made substantial progress in migrating
to the new stack. This was the central initial use case that drove initial work.
The transformation is not complete but is well in hand.</p></li>
<li><p>Redundant RADAR acquisition through two national hubs (spring 2016)</p></li>
<li><p>National Ninjo (main workstation for Forecasters) Dissemination (summer 2016)</p></li>
<li><p>Unified RADAR Processing (application to transform volume scans to products)
data flows (through 2017)</p></li>
</ul>
<p>Use cases in the video, but not yet realized:</p>
<ul class="simple">
<li><p>End user usage. A few trials were completed in early in 2017 leading to some
review, refactoring, and now retesting.</p></li>
<li><p>Data sets from sequencers have been waiting for end user use cases to be
improved.</p></li>
<li><p>Reporting to sources who consumed their products. The feature is
designed but further implementation, testing and careful deployment is needed.</p></li>
<li><p>Multi-pump routing by source-routing. Currently routing through multiple
pumps is done by pump administrators, rather than end users. Without end-user
on-boarding, routing by sources was a low priority.</p></li>
<li><p>The mesh interconnect model envisioned has not seen an appropriate use
case.</p></li>
</ul>
<p>Unanticipated use cases implemented:</p>
<ul class="simple">
<li><p>GTS data exchange: CMC &lt;-&gt; NWS. NWS requested a change in connectivity
in December 2015.</p></li>
<li><p>HPC mirroring (to be completed in Spring 2018).</p></li>
<li><p>Legacy Application 7-way replication (for SPCs) implemented last year.</p></li>
<li><p>GOES-R acquisition (live as of January 2018).</p></li>
</ul>
<p>Details to follow.</p>
</section>
<section id="central-data-flows">
<h2><a class="toc-backref" href="#id3" role="doc-backlink">Central Data Flows</a><a class="headerlink" href="#central-data-flows" title="Link to this heading"></a></h2>
<p>The slide below corresponds to deployed daily data flows in support of
Environment Canada, mostly for operational weather forecasting, in place since
January 2018.</p>
<img alt="../../_images/E-services_data-volume_pas.png" src="../../_images/E-services_data-volume_pas.png" />
<p>Sarracenia is being used operationally to acquire about four terabytes of
observations from automated weather observing systems, weather RADARS which
deliver data directly to our hubs, and international peer operated public file
services, which provide satellite imagery and numerical products from other
national weather centres.</p>
<p>Within the main high performance computing (HPC) data centre, there are two
supercomputers, two site stores, and two pre- and post-processing clusters.
Should a component in one chain fail, the other can take over. The input
data is sent to a primary chain, and then processing on that chain is mirrored,
using sarracenia to copy the data to the other chain. That’s about 16 of the
25 terabytes of the data centre traffic in this diagram.</p>
<p>A distillation of the data acquired, and the analysis and forecasts done in HPC,
is the 7 terabytes at the top right, that is sent to the seven regional
Storm Prediction Centres (SPCs).</p>
<p>The products of the SPCs and the central HPC are then shared with the public
and partners in industry, academia and other governments.</p>
</section>
<section id="weather-application-flows">
<h2><a class="toc-backref" href="#id4" role="doc-backlink">Weather Application Flows</a><a class="headerlink" href="#weather-application-flows" title="Link to this heading"></a></h2>
<p>FIXME: picture?</p>
<p>There is a number (perhaps a dozen?) of older applications (the most prominent ones
being BULLPREP and Scribe) used for decades in the Storm Prediction Centres
to create forecast and warning products. These applications are based on a file
tree that they read and write. Formerly, each application had its own backup
strategy with one of the six other offices and bi-lateral arrangements were made
to copy specific data among the trees.</p>
<p>In January 2017, complete 7-way replication of the state file trees of the
applications was implemented so that all offices have copies of files in
real-time. This is accomplished using Sarracenia through the eastern hub. Any
forecast office can now take over work of any product for any other, with no specific
application work needed at all.</p>
</section>
<section id="goes-r-acquisition">
<h2><a class="toc-backref" href="#id5" role="doc-backlink">GOES-R Acquisition</a><a class="headerlink" href="#goes-r-acquisition" title="Link to this heading"></a></h2>
<p>Acquisition of simulated and real GOES-R products from NOAA’s PDA, as well as
via local downlinks at one location (eventually to become two) was entirely
mediated by Sarracenia. The operational deployment of GOES-R happened in the
first week of January, 2018.</p>
</section>
<section id="hpc-acquisition-feeds">
<h2><a class="toc-backref" href="#id6" role="doc-backlink">HPC Acquisition Feeds</a><a class="headerlink" href="#hpc-acquisition-feeds" title="Link to this heading"></a></h2>
<p>FIXME: picture?</p>
<p>The supercomputing environment was entirely replaced in 2017. As part of that,
the client Environmental Data acquisition suite (ADE in French) was
refactored to work with much higher performance than previously, and to accept
Sarracenia feeds directly, rather than accepting feeds from previous generation
pump (Sundew).  The volume and speed of data acquisition has been substantially
improved as a result.</p>
</section>
<section id="radar-data-flows">
<h2><a class="toc-backref" href="#id7" role="doc-backlink">RADAR Data Flows</a><a class="headerlink" href="#radar-data-flows" title="Link to this heading"></a></h2>
<p>If we begin with RADAR data acquisition as an example, individual RADAR systems
use FTP and/or SFTP to send files to eastern and western communications hubs.
Those hubs run the directory watching component (sr_watch) and determine
checksums for the volume scans as they arrive. The Unified RADAR Processing
(URP) systems sr_subscribes to a hub, listening for new volume scans, and
downloads new data as soon as they are posted. URP systems then derive new
products and advertise them to the local hub using the sr_post component.
In time, we hope to have a second URP fully at the western hub.</p>
<p>In regional offices, the NinJo visualization servers download volume scans and
processed data from URP using identical subscriptions, pulling the data from
whichever national hub makes the data available first. The failure of a
national hub is transparent for RADAR data in that the volume scans will be
downloaded from the other hub, and the other URP processor will produce the
products needed.</p>
<a class="reference internal image-reference" href="../../_images/RADAR_DI_LogicFlow_Current.gif"><img alt="../../_images/RADAR_DI_LogicFlow_Current.gif" src="../../_images/RADAR_DI_LogicFlow_Current.gif" style="width: 1065.2px; height: 812.8000000000001px;" /></a>
<p>Each site has multiple Ninjo servers. We use http-based file servers, or web accessible folders to serve data.
This allows easy integration of web-proxy caches, which means that only the first Ninjo server to request data
will download from the national hub. Other Ninjo servers will get their data from the local proxy cache.
The use of Sarracenia for notifications when new products are available is completely independent of the
method used to serve and download data. Data servers can be implemented with a wide variety of tools
and very little integration is needed.</p>
</section>
<section id="hpc-mirroring">
<h2><a class="toc-backref" href="#id8" role="doc-backlink">HPC Mirroring</a><a class="headerlink" href="#hpc-mirroring" title="Link to this heading"></a></h2>
<p>All through 2017, work was proceeding to implement high speed mirroring between the supercomputer site stores
to permit failover. That work is now in a final deployment phase, and should be in operations by spring 2018.
For more details see: <a class="reference external" href="HPC_Mirroring_Use_Case.html">HPC Mirroring Use Case</a></p>
</section>
<section id="application-changes-in-2017">
<h2><a class="toc-backref" href="#id9" role="doc-backlink">Application Changes in 2017</a><a class="headerlink" href="#application-changes-in-2017" title="Link to this heading"></a></h2>
<p>Development of Sarracenia had been exploratory over a number of years. The use cases initially attacked
were those with a high degree of expert involvement. It proceeded following the minimum viable product (MVP)
model for each use case, acquiring features to deal with next use case prior to deployment. In 2016,
national deployment of NinJo and the Weather.</p>
<p>Expanded use cases explored:</p>
<ul class="simple">
<li><p>Mirroring: Prior to this use case, Sarracenia was used for raw data dissemination without regard for
permissions, ownership, symbolic links, etc…  For the mirroring use case, exact metadata
replication was a surprisingly complex requirement.</p></li>
<li><p>C-implementation: In exploring large scale mirroring, it became obvious that for sufficiently large
trees (27 Million files), the only practical method available was the use of a C shim library.
Having all user codes invoke a Python3 script is complete nonsense in an HPC environment, so
it was necessary to implement a C version of Sarracenia posting code for use by the shim library.
Once the C implementation was begun, it was only a little additional work to implement a C version
of sr_watch (called sr_cpost) which was much more memory and CPU efficient than the Python original.</p></li>
<li><p>Node.js implementation: A client of the public datamart decided to implement enough of Sarracenia
to download warnings in real-time.</p></li>
<li><p>The application was refactored to maximize consistency through code reuse, reducing about 20% of
the code size at one point. The code returned to the initial size when new features were added,
but it remains quite compact at less than 20 kloc.</p></li>
<li><p>End-user usage: All of the deployments thus far are implemented by analysts with a deep understanding
of Sarracenia, and extensive support and background. This year, we went through several iterations
of having users deploy their flows, collecting feedback and then making it easier for end users at
the next iteration. Many of these changes were <em>breaking</em> changes, in that options and ways or
working were still prototypes and required revision.</p></li>
</ul>
<p>Changes to support end user usage:</p>
<ul class="simple">
<li><p>Exchanges were an administrator-defined resource. Permission model changed such that users can now declare exchanges.</p></li>
<li><p>Previously, one had to look on web sites to find examples. Now, the <em>list</em> command displays many examples included with the package.</p></li>
<li><p>It was hard to find where to put settings files. The <em>list/add/remove/edit</em> commands simplify that.</p></li>
<li><p>In each plugin entry point, one had to modify different instance variables, was refactored for consistency
across all of them (on_msg, on_file, on_part, on_post, do_download, do_send, etc…).</p></li>
<li><p>Partitioning specifications were arcane and were replaced with the
<em>blocksize</em> option, with only three possibilities: 0, 1, many.</p></li>
<li><p>Routing across multiple pumps was arcane. The original algorithm was
replaced by a simpler one with some smarter defaults. Users can now usually
ignore it.</p></li>
<li><p>A much more elegant plugin interface is available to have multiple routines that
work together, specified in a single plugin.</p></li>
<li><p>Previously, only advertised on web servers relative to the root URL. Now,
non-root base URL support was added.</p></li>
</ul>
<p>The only major operational feature introduced in 2017 was
<strong>save/restore/retry</strong>: if a destination has a problem, there is
substantial risk of overloading AMQP brokers by letting queues of products to
transfer grow into millions of entries. Functionality to efficiently (in
parallel) offload broker queues to local disk was implemented to address
this. At first, recovery needed to be manually triggered (restore) but by
the end of the year, an automated recovery (retry) mechanism was working its
way to deployment, which will reduce requirements for oversight and
intervention in operations.</p>
</section>
<section id="coming-in-2018">
<h2><a class="toc-backref" href="#id10" role="doc-backlink">Coming in 2018</a><a class="headerlink" href="#coming-in-2018" title="Link to this heading"></a></h2>
<p>As of release 2.18.01a5, all of the use cases targeted have been explored and
reasonable solutions are available, so there should be no further changes to
the existing configuration language or options. No changes to existing
configuration settings are planned. Some minor additions may still occur,
but not at the cost of breaking any existing configurations. The core
application is now complete.</p>
<p>Expect in early 2018 for the last alpha package release and
for subsequent work to be on a beta version with a target of a much more
long-lived stable version some time in 2018.</p>
<ul class="simple">
<li><p>HPC mirroring use case deployment will be completed.</p></li>
<li><p>The Permanent File Depot (PFD) use case will be deployed. Currently, this
is used to cover a short time horizon. One can extend it arbitrarily into the
past by persisting the time-based tree to nearline storage. In development
since 2016, gradually progressing.</p></li>
<li><p>Improve deployment consistency: The changes in 2017 were confusing for the
expert analysts, as significant changes in details occurred across versions.
Different deployments currently use different operational versions, and most
issues arising in operations are addressed by the existing code, but are not
yet deployed to that use case. In 2018, we will revisit early deployments to
bring them up to date.</p></li>
<li><p>Continued improvement in pre-deployment testing.</p></li>
<li><p>The Sarrasemina indexing tool, which facilitates finding feeds, to be deployed
to assist onboarding.</p></li>
<li><p>Improved onboarding documentation. Reference materials are thorough, but
introductory quick-start and <em>gateway</em> oriented materials need work.
French translations are also needed.</p></li>
<li><p>Reporting: While reporting was baked in from the start, it proved to be very
expensive, and so deployments to date have omitted it. Now that deployment
loads are quieting down, this year should allow us to add real-time report
routing to deployed configurations. There is no functionality to develop,
as everything is already in the application, but mostly not used. Use may
uncover additional issues.</p></li>
<li><p>Pluggable checksum algorithms. Currently checksum algorithms are baked into
the implementations. There is a need to support plugins to support
user-defined checksum algorithms (expected in 2.18.02a1).</p></li>
<li><p>Continued progressive replacement of legacy application configurations
(RPDS, Sundew).</p></li>
<li><p>Continued adaptation of applications to Sarracenia (DMS, GOES-R).</p></li>
<li><p>Deployment of additional instances:  flux.weather.gc.ca,
hpfx.collab.science.gc.ca, etc…</p></li>
<li><p>Continued work on the corporate approval and funding of the western hub (aka.
Project Alta).</p></li>
</ul>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="index.html" class="btn btn-neutral float-left" title="History" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="Evolution.html" class="btn btn-neutral float-right" title="History/Context of Sarracenia" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, Shared Services Canada, Government of Canada, GPLv2.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>