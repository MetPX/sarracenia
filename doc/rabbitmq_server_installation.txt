

HISTORIQUE DES RÉVISIONS


Table des matières

1.	INTRODUCTION	
2.	RABBITMQ-SERVER  INSTALLATION	
3.	RABBITMQ-SERVER  INSTALLATION  EN  CLUSTER	
4.	RABBITMQ-SERVER  INSTALLATION  DE LDAP	
5.	UTILISATION DE AMQP SUR DD (DDI, DD.BETA)	
6.	UTILISATION DE AMQP AVEC URP, BUNNY, PDS-OP
6.1	DE URP-1/2 ANNONCER A BUNNY-OP QU'UN PRODUIT EST PRÊT
6.2	BUNNY-OP ET  DD_DISPATCHER.PY
6.3	PDS-OP  RÉCEPTIONS DE MESSAGES DISPATCH, WGET DE PRODUITS RADARS
6.4	VÉRIFICATION / TROUBLESHOOTING

1. 
Introduction


AMQP est l'acronyme de  Advanced Message Queuing Protocol.  C'est la définition d'un protocol qui vient du besoin de normer un système d'échange de message asynchrone.  Dans le jargon AMQP on parlera de producteurs de messages, de consommateurs de messages et de courtier (broker).

2. RABBITMQ-SERVER  installation

Sur nos machines qui doivent traitées des messages AMQP, on installe le broker, en installant le package rabbitmq-server_3.3.5-1_all.deb.  L'installation de base se fait comme suit sur toutes nos machines :

# installing package taken on the rabbitmq homepage
# rabbitmq-server version > 3.3.x  requise pour utilisation de ldap pour verification des passwords seulement

apt-get install erlang-nox
dpkg -i /tmp/rabbitmq-server_3.3.5-1_all.deb

# create anonymous user
# password ********* provided in patates
#                                          conf write read
rabbitmqctl add_user anonymous *********
rabbitmqctl set_permissions -p / anonymous   "^xpublic|^amq.gen.*$|^cmc.*$"     "^amq.gen.*$|^cmc.*$"    "^xpublic|^amq.gen.*$|^cmc.*$"
rabbitmqctl list_user_permissions anonymous

# create feeder user
# password ********* provided in patates
#                                       conf write read
rabbitmqctl add_user feeder ********
rabbitmqctl set_permissions -p / feeder  ".*"  ".*"  ".*"
rabbitmqctl list_user_permissions feeder

# create administrator user 
# password ********* provided in patates

rabbitmqctl add_user root   *********
rabbitmqctl set_user_tags root administrator

# takeaway administrator privileges from guest
rabbitmqctl set_user_tags guest
rabbitmqctl list_user_permissions guest
rabbitmqctl change_password guest *************

# list users 
rabbitmqctl list_users
 

# enabling management web application 
# this is important since sr_rabbit uses this management facility/port access
# to retrieve some important info

rabbitmq-plugins enable rabbitmq_management
/etc/init.d/rabbitmq-server restart



3. RABBITMQ-SERVER  installation  en  cluster

Sur les bunny ont a opte pour une installation en cluster. Pour ce faire on suit les instructions suivantes :

        Arreter rabbitmq-server sur tous les noeuds....

        /var/lib/rabbitmq/.erlang.cookie  same on all nodes

        on each node restart  /etc/init.d/rabbitmq-server stop/start

        on one of the node

        rabbitmqctl stop_app
        rabbitmqctl join_cluster rabbit@"other node"
        rabbitmqctl start_app
        rabbitmqctl cluster_status


       # having high availability queue...
       # here all queues that starts with "cmc." will be highly available on all the cluster nodes

       rabbitmqctl set_policy ha-all "^cmc\." '{"ha-mode":"all"}'



4. RABBITMQ-SERVER  installation  de ldap

Sur les serveurs ou on veut avoir un authentification utilisant on suit les instructions suivantes :


         rabbitmq-plugins enable rabbitmq_auth_backend_ldap

         # replace username by ldap username
         # clear password (will be verified through the ldap one)
         rabbitmqctl add_user username aaa
         rabbitmqctl clear_password username
         rabbitmqctl set_permissions -p / username "^xpublic|^amq.gen.*$|^cmc.*$" "^amq.gen.*$|^cmc.*$" "^xpublic|^amq.gen.*$|^cmc.*$"


Et on configure les services de LDAP dans le fichier de configuration du rabbitmq-server :
(old ldap-dev test config that worked than...)

    
cat /etc/rabbitmq/rabbitmq.config
[ {rabbit, [{auth_backends, [ {rabbit_auth_backend_ldap,rabbit_auth_backend_internal}, rabbit_auth_backend_internal]}]},
  {rabbitmq_auth_backend_ldap,
   [ {servers,               ["ldap-dev.cmc.ec.gc.ca"]},
     {user_dn_pattern,       "uid=${username},ou=People,ou=depot,dc=ec,dc=gc,dc=ca"},
     {use_ssl,               false},
     {port,                  389},
     {log,                   true},
     {network,               true},
    {vhost_access_query,    {in_group,
                             "ou=${vhost}-users,ou=vhosts,dc=ec,dc=gc,dc=ca"}},
    {resource_access_query,
     {for, [{permission, configure, {in_group, "cn=admin,dc=ec,dc=gc,dc=ca"}},
            {permission, write,
             {for, [{resource, queue,    {in_group, "cn=admin,dc=ec,dc=gc,dc=ca"}},
                    {resource, exchange, {constant, true}}]}},
            {permission, read,
             {for, [{resource, exchange, {in_group, "cn=admin,dc=ec,dc=gc,dc=ca"}},
                    {resource, queue,    {constant, true}}]}}
           ]
     }},
    {tag_queries,           [{administrator, {constant, false}},
                             {management,    {constant, true}}]}
   ]
  }
].


5. Utilisation de AMQP sur DD (DDI, DD.BETA)

On (Peter) a voulu faire une implémentation de AMQP dans METPX.  Pour ce faire, on utilise la librairie python-amqplib qui implémente la fonctionnalité nécessaire à AMQP en python. On a ainsi développé un pxSender de type amqp  qui est le producteur de messages de même qu'un pxReceiver de type amqp qui sert de consommateur de messages.  Comme broker, on utilise rabbitmq-server qui est un package standard debian d'un broker AMQP.

 Un pxSender de type amqp, lit le contenu d'un fichier dans sa queue, en fait un message auquel il attache un "topic" et envoi le tout au broker.  Un pxReceiver de type amqp va annoncer au broker le "topic" pour lequel il est intéressé à recevoir des messages, et celui-ci lui enverra chaque message correspondant a son choix. 

 Comme un message peut être n'importe quoi, au niveau du pxSender, on a aussi attaché le nom du fichier d'ou le message provient. Ainsi dans notre pxReceiver, on peut insérer le contenu du message dans le nom de fichier correspondant. Cette astuce est inutile seulement pour les échanges amqp entre un sender et un receiver amqp...

5.1 Notifications pour DD 

On a trouve dans AMQP une opportunité pour annoncer les produits quand ils arrivent sur DD. Ainsi un usager au lieu de sans cesse vérifier si un produit est présent sur DD pour le télécharger, il pourrait s'abonner (AMQP topic) pour recevoir un message (le url du produit) qui serait émis seulement a la livraison du produit sur DD.  On ne ferait pas cette exercice pour les bulletins... mais pour les autres produits (grib,images... etc)

 Pour implémenter ceci, on a utiliser une possibilité de pxSender, le sender_script. On a écrit un script   sftp_amqp.py  qui fait les livraisons vers  DD et pour chaque produit, il cree un fichier contenant le URL sous lequel le produit sera présenté...Voici le debut de la configuration de  wxo-b1-oper-dd.conf


type script
send_script sftp_amqp.py

# connection info
protocol    ftp
host        wxo-b1.cmc.ec.gc.ca
user        wxofeed
password    **********
ftp_mode    active

noduplicates false

# no filename validation (pds format)
validation  False

# delivery method
lock  umask
chmod 775
batch 100


On voit dans cette config que toute les informations pour un sender de type  single-file sont la. Mais parce que le type est script... et qu'est fournit le send_script  sftp_amqp.py,  on est en mesure d'instrumenter notre sender pour faire plus...  


Le fichier contenant le URL est place sous le txq d'un sender AMQP
 /apps/px/txq/dd-notify-wxo-b1  pour que la notification AMQP soit faite.
 Pour envoyer les fichiers de cette queue, un sender a été écrit  dd-notify-wxo-b1.conf   dont voici la config:



type amqp

validation False
noduplicates False

protocol amqp
host wxo-b1.cmc.ec.gc.ca
user feeder
password ********

exchange_name cmc  
exchange_key  exp.dd.notify.${0}
exchange_type topic

reject ^ensemble.naefs.grib2.raw.*

accept ^(.*)\+\+.*


Ici encore, la clé pour le topic contient une partie programmée. La partie ${0} contient l'arborescence ou le produit est place sur dd... Par exemple, voici une ligne de log de 
dd-notify-wxo-b1.log

2013-06-06 14:47:11,368 [INFO] (86 Bytes) Message radar.24_HR_ACCUM.GIF.XSS++201306061440_XSS_24_HR_ACCUM_MM.gif:URP:XSS:RADAR:GIF::20130606144709  delivered (lat=1.368449,speed=168950.887119)

Et donc la clé serait                    exp.dd.notify.radar.24_HR_ACCUM.GIF.XSS 
et l'emplacement du fichier        http://dd1.weather.gc.ca/radar/24_HR_ACCUM/GIF/XSS
et le URL complet dans le message  http://dd1.weather.gc.ca/radar/24_HR_ACCUM/GIF/XSS/201306061440_XSS_24_HR_ACCUM_MM.gif



5.2 Utilitaires installés sur les serveurs de DD


Quand un client se connecte  au broker  (rabbitmq-server) il doit crée une queue et
l'attacher a un exchange.  On peut donner a cette queue l'option qu'elle s'autodétruise quand elle n'est plus utilisée  ou  qu'elle soit conservée et qu'elle continue d'empiler des produits si le client est hors connexion.  En général, on voudrait que la queue soit préservée et ainsi  la reprise de connexion  redémarre la collecte de produits sans perte. 

queue_manager.py

Le rabbitmq-server ne détruira jamais une queue qui a été crée par un client si elle n'est pas en mode  auto-delete (encore moins si elle est cree avec durable).  Cela peut causer un problème par exemple, un client qui développe un processus, peut changer plusieurs fois d'idée et créés sur le serveur une multitude de queues qui ne seront jamais utilisées.   On a donc créé un script   queue_manager.py   qui vérifie si les queues non utilisees on plus de   X  produits en attente ou  prennent plus de Y Mbytes...Si oui, elles sont détruites par le script.  Au moment de l'écriture de ce document,  les limites sont

25000 messages  et  50Mb.


dd-xml-inotify.py

Sur notre datamart public, il y a des produits qui ne proviennent pas directement de pds/px/pxatx.  Comme nos notifications sont fait a partir de la livraison du produit,  on a pas de messages pour ceux-ci.   C'est le cas des produits XML sous  les répertoires :
citypage_weather et marine_weather.   Pour palier a cette situation, le daemon 
dd-xml-inotify.py a été  créé et installé.   Ce script python utilise inotify pour surveiller la modification des produits sous leurs répertoires.  Si un produit est modifié ou ajouté, une notification amqp est envoyé au serveur.  Ainsi tous les produits du datamart sont couvert pas l'envoi de message.  


6. Utilisation de AMQP avec URP, BUNNY, PDS-OP

*** s'applique aussi a dev ...


6.1 De URP-1/2 annoncer a BUNNY-OP qu'un produit est prêt.

Sur urp-1/2 un metpx roule le sender  amqp_expose_db.conf qui annonce qu'un produit vient d'arriver dans la db de metpx avec un message de la forme :

"Md5sum of product name"  file'size  url dbname

Ex.:

a985c32cbdee8af2ab5d7b8f6022e781 498081 http://urp-1.cmc.ec.gc.ca/ db/20150120/RADAR/URP/IWA/201501201810~~PA,60,10,PA_PRECIPET,MM_HR,MM:URP:IWA:RADAR:META::20150120180902

Ces messages AMQP sont envoyés vers le serveur rabbitmq sur bunny-op avec un exchange key qui commence par v00.urp.input suivit par convention par le path a partir de db  avec les '/' remplacer par '.'


Remarquer que urp-1/2 roule apache et que le produit annoncé est dans la db de metpx et est visible a partir du URL du message


6.2 BUNNY-OP et  dd_dispatcher.py

bunny-op est un vip qui vit sur bunny1-op ou sur bunny2-op.
C'est avec keepalived qu'on s'assure que ce vip reside sur un des bunny-op.
On test aussi que rabbitmq-server roule sur ce meme serveur. La partie de configuration de keepalived qui s'occupe du vip est :

#=============================================
# vip bunny-op 142.135.12.59 port 5672
#=============================================

vrrp_script chk_rabbitmq {
        script "killall -0 rabbitmq-server"
        interval 2
}

vrrp_instance bunny-op {
        state BACKUP
        interface eth0
        virtual_router_id 247
        priority 150
        track_interface { 
                eth0 
        }
        advert_int 1
        preempt_delay 5
        authentication {
                auth_type PASS
                auth_pass bunop
        }
        virtual_ipaddress {
# bunny-op
                142.135.12.59 dev eth0
        }
        track_script {
                chk_rabbitmq
        }
}

Les rabbitmq-servers sur ces machines sont installés en cluster.
On a mis une haute disponibilités sur les queues débutant par cmc.*.
Sur chacune des machines roulent l'utilitaire dd_dispatcher.py.
Ce programme vérifie si le vip bunny-op et procèdera a son travail seulement
sur le serveur ou le vip vit. (Si il y a un switch, auto détection en 5 secondes et
les queues restent inchangées) 

L'utilitaire dd_dispatcher.py  s'abonne aux messages v00.urp.input.# et reçoit donc les messages des 2 serveurs opérationnels URP.  A la réception d'un premier produit, le md5dum du produit est placé dans un cache et le message est réexpédié  mais cette fois avec comme exchange key  v00.urp.notify. Si un autre message arrive de v00.urp.input avec le même md5sum que le premier, il est ignoré, donc les produits annonces a partir de l'exchange key v00.urp.notify sont unique et représente le premier arrivé d'entre les 2 URP opérationnels.  

6.3 PDS-OP  réceptions de messages dispatch, wget de produits radars

Sur pds-op, un receiver pull_urp, roule le fx_script  pull_amqp_wget.py.  Dans ce script, la commande suivante :

                  # shared queue : each pull receive 1 message (prefetch_count=1)
                  self.channel.basic_qos(prefetch_size=0,prefetch_count=1,a_global=False)

fait que la distribution des messages v00.urp.notify seront distribués équitablement a travers les 5 serveurs sous pds-op. On garantie donc un pull distribué. Pour chaque message  de la forme :

a985c32cbdee8af2ab5d7b8f6022e781 498081 http://urp-1.cmc.ec.gc.ca/ db/20150120/RADAR/URP/IWA/201501201810~~PA,60,10,PA_PRECIPET,MM_HR,MM:URP:IWA:RADAR:META::20150120180902

le url est rebâtit a partir des 2 derniers champs du message et un wget du produit est fait et placé dans la queue du receiver qui est ensuite ingéré/routé de façon ordinaire.








6.4 Vérification / Troubleshooting 

Dans l'ordre de la production 

1- Sur urp-1/2 :
- Vérifier que les produits radars sont bien générés sur urp-1/2
- Vérifier que les notifications sont générés sur urp-1/2  /apps/px/log/tx_amqp_expose_db.log
2- Sur bunny1/2-op
- Vérifier ou réside bunny-op
- Vérifier les logs de dd_dispatcher.py 
/var/log/dd_dispatcher_xxxx.log ou xxxx est le pid du process
3- Sur pds-op
- Vérifier le pull_urp   

Repartir les processus qui ne fonctionne pas bien devrait régler les problèmes en général…   On ajoutera plus de détails ici a mesure que des problèmes seront rencontrés et corrigés. 
